# 基本网络结构

> stride=2：输出减半
>
> padding=2:输出输出大小一致
>
> > kernel size = 3
> >
> > stride = 1 
> >
> > 高宽不变

## LeNet

![image-20210609092427652](D:\notes\images\image-20210609092427652.png)

![image-20210609150320720](D:\notes\images\image-20210609150320720.png)

> 5x5 Conv(6),pad 2:意为 5x5的卷积核，输出通道为6，填充为2，由（f-1）/2 == 2可知，为same填充，即输入输出大小不变
>
> `速记：两卷平，三接`

```python
class Reshape(torch.nn.Module): 
    def forward(self, x):
        return x.reshape(-1, 1, 28, 28)


net = torch.nn.Sequential(
    Reshape(),
    nn.Conv2d(1, 6, kernel_size=5, padding=2), nn.Sigmoid(),
    nn.AvgPool2d(kernel_size=2, stride=2),
    nn.Conv2d(6, 16, kernel_size=5), nn.Sigmoid(),
    nn.AvgPool2d(kernel_size=2, stride=2),
    nn.Flatten(),
    nn.Linear(16 * 5 * 5, 120), nn.Sigmoid(),
    nn.Linear(120, 84), nn.Sigmoid(),
    nn.Linear(84, 10)
)
```

## AlexNet

![image-20210610102314561](D:\notes\images\image-20210610102314561.png)

> `速记：两卷最池，三卷最池，三接`

```python
net = nn.Sequential(
    # 这里，我们使用一个11*11的更大窗口来捕捉对象。
    # 同时，步幅为4，以减少输出的高度和宽度。
    # 另外，输出通道的数目远大于LeNet
    nn.Conv2d(1, 96, kernel_size=11, stride=4, padding=1), nn.ReLU(),
    nn.MaxPool2d(kernel_size=3, stride=2),
    # 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数
    nn.Conv2d(96, 256, kernel_size=5, padding=2), nn.ReLU(),
    nn.MaxPool2d(kernel_size=3, stride=2),
    # 使用三个连续的卷积层和较小的卷积窗口。
    # 除了最后的卷积层，输出通道的数量进一步增加。
    # 在前两个卷积层之后，池化层不用于减少输入的高度和宽度
    nn.Conv2d(256, 384, kernel_size=3, padding=1), nn.ReLU(),
    nn.Conv2d(384, 384, kernel_size=3, padding=1), nn.ReLU(),
    nn.Conv2d(384, 256, kernel_size=3, padding=1), nn.ReLU(),
    nn.MaxPool2d(kernel_size=3, stride=2), nn.Flatten(),
    # 这里，全连接层的输出数量是LeNet中的好几倍。使用dropout层来减轻过度拟合
    nn.Linear(6400, 4096), nn.ReLU(), nn.Dropout(p=0.5),
    nn.Linear(4096, 4096), nn.ReLU(), nn.Dropout(p=0.5),
    # 最后是输出层。由于这里使用Fashion-MNIST，所以用类别数为10，而非论文中的1000
    nn.Linear(4096, 10)
)
```

## VGG

> VGG使用可重复使用的`卷积块`来构建深度卷积神经网络
> 不同的卷积块个数和超参数可以得到不同复杂度的变种

![image-20210612094954119](D:\notes\images\image-20210612094954119.png)

> `n卷最池，n块,三接`

```python
def vgg_block(num_convs, in_channel, out_channel):
    layer = []
    for _ in range(num_convs):
        layer.append(nn.Conv2d(in_channel, out_channel, kernel_size=3, padding=1))
        layer.append(nn.ReLU())
        in_channel = out_channel
    layer.append(nn.MaxPool2d(2, stride=2))
    return nn.Sequential(*layer)


conv_arch = ((1, 64), (1, 128), (2, 256), (2, 512), (2, 512))


def vgg(conv_arch):
    in_channel = 1
    conv_a = []
    for num, out_channel in conv_arch:
        conv_a.append(vgg_block(num, in_channel, out_channel))
        in_channel = out_channel

    return nn.Sequential(*conv_a, nn.Flatten(),
                         nn.Linear(out_channel * 7 * 7, 4096), nn.ReLU(), nn.Dropout(0.5),
                         nn.Linear(4096, 4096), nn.ReLU(), nn.Dropout(0.5),
                         nn.Linear(4096, 10))
```

## NIN

> ●无全连接层
> ●交替使用NiN块和步幅为2的最大池化层逐步减小高宽和增大通道数
> ●最后使用全局平均池化层得到输出其输入通道数是类别数

![image-20210612093443941](D:\notes\images\image-20210612093443941.png)

> `三卷121最池，n块，全平均`

```python
from torch import nn


def NIN_block(in_channel, out_channel, kernel_size, stride, padding):
    return nn.Sequential(
        nn.Conv2d(in_channel, out_channel, kernel_size, stride, padding), nn.ReLU(),
        nn.Conv2d(out_channel, out_channel, kernel_size=1), nn.ReLU(),
        nn.Conv2d(out_channel, out_channel, kernel_size=1), nn.ReLU()
    )


def Nin():
    return nn.Sequential(
        NIN_block(1, 96, 11, 4, 0),
        nn.MaxPool2d(3, 2),
        NIN_block(96, 256, 5, 1, 2),
        nn.MaxPool2d(3, 2),
        NIN_block(256, 384, 3, 1, 1),
        nn.MaxPool2d(3, 2),
        nn.Dropout(0.5),
        NIN_block(384, 10, 3, 1, 1),
        nn.AdaptiveAvgPool2d((1, 1)),
        nn.Flatten()
    )

```

## GoogLeNet

> Inception块用4条有不同超参数的卷积层和池化层的路来抽取不同的信息,，然后在输出通道维合并
>
> 它的一个主要优点是模型参数小，计算复杂度低

![image-20210611165404621](D:\notes\images\image-20210611165404621.png)

![image-20210611165417735](D:\notes\images\image-20210611165417735.png)

```python
import torch
from torch import nn
from torch.nn import functional as F
from d2l import torch as d2l

class Inception(nn.Module):
    # `c1`--`c4` 是每条路径的输出通道数
    def __init__(self, in_channels, c1, c2, c3, c4, **kwargs):
        super(Inception, self).__init__(**kwargs)
        # 线路1，单1 x 1卷积层
        self.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=1)
        # 线路2，1 x 1卷积层后接3 x 3卷积层
        self.p2_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1)
        self.p2_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)
        # 线路3，1 x 1卷积层后接5 x 5卷积层
        self.p3_1 = nn.Conv2d(in_channels, c3[0], kernel_size=1)
        self.p3_2 = nn.Conv2d(c3[0], c3[1], kernel_size=5, padding=2)
        # 线路4，3 x 3最大池化层后接1 x 1卷积层
        self.p4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)
        self.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=1)

    def forward(self, x):
        p1 = F.relu(self.p1_1(x))
        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))
        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))
        p4 = F.relu(self.p4_2(self.p4_1(x)))
        # 在通道维度上连结输出
        return torch.cat((p1, p2, p3, p4), dim=1)

b1 = nn.Sequential(nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),
                   nn.ReLU(), nn.MaxPool2d(kernel_size=3, stride=2,
                                           padding=1))
b2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=1), nn.ReLU(),
                   nn.Conv2d(64, 192, kernel_size=3, padding=1),
                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))
b3 = nn.Sequential(Inception(192, 64, (96, 128), (16, 32), 32),
                   Inception(256, 128, (128, 192), (32, 96), 64),
                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))
b4 = nn.Sequential(Inception(480, 192, (96, 208), (16, 48), 64),
                   Inception(512, 160, (112, 224), (24, 64), 64),
                   Inception(512, 128, (128, 256), (24, 64), 64),
                   Inception(512, 112, (144, 288), (32, 64), 64),
                   Inception(528, 256, (160, 320), (32, 128), 128),
                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))
b5 = nn.Sequential(Inception(832, 256, (160, 320), (32, 128), 128),
                   Inception(832, 384, (192, 384), (48, 128), 128),
                   nn.AdaptiveAvgPool2d((1, 1)), nn.Flatten())

net = nn.Sequential(b1, b2, b3, b4, b5, nn.Linear(1024, 10))
```

## ResNet

> 问题一：累加Conv会导致在反向传播过程中 x > 1 梯度爆炸，x < 1 梯度消失
>
> > 解决方案
>>
> > 1.权重初始化
> >
> > 2.数据标准化
> >
> > 3.batch norm

### 残差块

![image-20210613173317279](D:\notes\images\image-20210613173317279.png)

> 左图虚线框中的部分需要直接拟合出该映射 f(x) ，而右图虚线框中的部分则需要拟合出残差映射 f(x)−x 。 残差映射在现实中往往更容易优化。	 
>
> 这样的设计要求 2 个卷积层的输出与输入形状一样，从而可以相加。 如果想改变通道数，就需要引入一个额外的 1×1 卷积层来将输入变换成需要的形状后再做相加运算

![image-20210614160316111](D:\notes\images\image-20210614160316111.png)



```python
import torch
from torch import nn


# 残差
class Residual(nn.Module):
    def __init__(self, in_channel, out_channel, use_1x1=False, stride=1):
        super().__init__()
        # kernel size = 3 padding = 1 高宽不变
        self.conv1 = nn.Conv2d(in_channel, out_channel, kernel_size=3, padding=1, stride=stride)
        self.bn1 = nn.BatchNorm2d(out_channel)
        self.ReLu = nn.ReLU(inplace=True)

        self.conv2 = nn.Conv2d(out_channel, out_channel, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm2d(out_channel)

        # 降低通道 kernel size 1x1
        if use_1x1:
            self.conv3 = nn.Conv2d(in_channel, out_channel, kernel_size=1, stride=stride)
        else:
            self.conv3 = None

    def forward(self, x):
        out = self.ReLu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))

        if self.conv3 is not None:
            x = self.conv3(x)

        return self.ReLu(x + out)


# stage
def resnetBlock(in_channel, out_channel, size, is_First=False):
    blk = []
    for i in range(size):
        # not (is_First = True) ==> False
        if i == 0 and not is_First:
            # 其他通道 高宽减半
            blk.append(Residual(in_channel, out_channel, use_1x1=True, stride=2))
        else:
            # 第一个通道时 输入输出大小一致
            blk.append(Residual(out_channel, out_channel))
    return blk


def restNet():
    b1 = nn.Sequential(
        nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),
        nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
    )
    # 四个残差块
    # 第一个残差块
    b2 = nn.Sequential(*resnetBlock(64, 64, 2, is_First=True))
    # 其他通道
    b3 = nn.Sequential(*resnetBlock(64, 128, 2))
    b4 = nn.Sequential(*resnetBlock(128, 256, 2))
    b5 = nn.Sequential(*resnetBlock(256, 512, 2))
    # 全局平均池化
    avg = nn.AdaptiveAvgPool2d((1, 1))
    # 全连接
    liner = nn.Linear(512, 10)

    return nn.Sequential(b1, b2, b3, b4, b5, avg, nn.Flatten(), liner)


X = torch.rand(size=(1, 1, 224, 224))
for layer in restNet():
    X = layer(X)
    print(layer.__class__.__name__, 'output shape:\t', X.shape)
```

## DenseNet稠密连接网络

![image-20210621190613329](D:\notes\images\image-20210621190613329.png)

> 和ResNet相比的区别：没用使用输出相加，而是在通道上连接
>
> DenseNet主要构建模块是稠密块（dense block）和过渡层（transition layer），前者定义输入和输出如何连接，后者用来控制通道数，使之不过大

### 稠密块

```python
from torch import nn
import torch


def conv_block(in_channel, out_channel):
    return nn.Sequential(
        nn.BatchNorm2d(in_channel),
        nn.ReLU(),
        nn.Conv2d(in_channel, out_channel, kernel_size=3, padding=1)
    )


# dense net block
class Dense_block(nn.Module):

    def __init__(self, num, in_channel, out_channel, ):
        super(Dense_block, self).__init__()
        net = []
        for i in range(num):
            in_C = in_channel + i * out_channel
            net.append(conv_block(in_C, out_channel))
        self.net = nn.ModuleList(net)
        self.out_channel = in_channel + num * out_channel

    def forward(self, x):
        for blk in self.net:
            y = blk(x)
            # 连结
            x = torch.cat((x, y), dim=1)
        return x
```

### 过渡层

> 每个稠密块都会使得通道数增加，使用过多会带来过于复杂的模型
>
> 使用1x1的卷积降低通道数，以及步幅为2平均池化层

```python
def transition_block(in_channel, out_channel):
    return nn.Sequential(
        nn.BatchNorm2d(in_channel),
        nn.ReLU(),
        nn.Conv2d(in_channel, out_channel, kernel_size=1),
        nn.AvgPool2d(kernel_size=2, stride=2)
    )
```

DenseNet模型

```python
b1 = nn.Sequential(
    nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),
    nn.BatchNorm2d(64),
    nn.ReLU(),
    nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
)

num_channel, growth = 64, 32
num_convs_in_dense = [4, 4, 4, 4]
blks = []

for i, num in enumerate(num_convs_in_dense):
    blks.append(Dense_block(num, num_channel, growth))
    # 上一个稠密块的输出通道
    num_channel += num * growth
    # 添加转换层
    if i != len(num_convs_in_dense) - 1:
        blks.append(transition_block(num_channel, num_channel // 2))
        num_channel = num_channel // 2
# 全部结构
net = nn.Sequential(
    b1, *blks, nn.BatchNorm2d(num_channel),
    nn.ReLU(), nn.AdaptiveAvgPool2d((1, 1)), nn.Flatten(),
    nn.Linear(num_channel, 10)
)
```













