## 自定义数据集

### 读取自定义图片文件夹

```python
train_imgs = torchvision.datasets.ImageFolder(os.path.join(data_dir, 'train'))
test_imgs = torchvision.datasets.ImageFolder(os.path.join(data_dir, 'test'))
# 读取后，默认按照文件夹顺序进行排序，由上到下0，1，2...
```

### 图片预处理

> 在训练期间，我们首先从图像中裁切随机大小和随机长宽比的区域，然后将该区域缩放为 224×224 输入图像。 在测试过程中，我们将图像的高度和宽度都缩放到 256 像素，然后裁剪中央 224×224 区域作为输入。 此外，对于三个 RGB（红、绿和蓝）颜色通道，我们 *标准化* 每个通道。 具体而言，通道的平均值将从该通道的每个值中减去，然后将结果除以该通道的标准差。

```python
# 使用三个RGB通道的均值和标准偏差，以标准化每个通道
normalize = torchvision.transforms.Normalize([0.485, 0.456, 0.406],
                                             [0.229, 0.224, 0.225])

train_augs = torchvision.transforms.Compose([
    torchvision.transforms.RandomResizedCrop(224),
    torchvision.transforms.RandomHorizontalFlip(),
    torchvision.transforms.ToTensor(), normalize])

test_augs = torchvision.transforms.Compose([
    torchvision.transforms.Resize(256),
    torchvision.transforms.CenterCrop(224),
    torchvision.transforms.ToTensor(), normalize])
```

## 迁移学习

>  将从 `源数据集` 学到的知识迁移到 `目标数据集`

### 微调

1. 在源数据集（例如 ImageNet 数据集）上预训练神经网络模型，即 *源模型*。
2. 创建一个新的神经网络模型，即 *目标模型*。这将复制源模型上的所有模型设计及其参数，但**输出层**除外。我们假定这些模型参数包含从源数据集中学到的知识，这些知识也将适用于目标数据集。我们还假设源模型的输出图层与源数据集的标签密切相关；因此不在目标模型中使用该图层。
3. 向目标模型添加输出图层，其输出数量是目标数据集中的类别数。然后随机初始化该层的模型参数。
4. 在目标数据集（如椅子数据集）上训练目标模型。输出图层将从头开始进行训练，而所有其他图层的参数将根据源模型的参数进行微调。

### 案例：热狗

```PYTHON
finetune_net = torchvision.models.resnet18(pretrained=True)# 下载预训练好的模型
finetune_net.fc = nn.Linear(finetune_net.fc.in_features, 2)# 重新定义全连接层
nn.init.xavier_uniform_(finetune_net.fc.weight)# 初始化模型参数

# 定义训练函数
# 如果 `param_group=True`，输出层中的模型参数将使用十倍的学习率
def train_fine_tuning(net, learning_rate, batch_size=128, num_epochs=5,
                      param_group=True):
    train_iter = torch.utils.data.DataLoader(
        torchvision.datasets.ImageFolder(os.path.join(data_dir, 'train'),
                                         transform=train_augs),
        batch_size=batch_size, shuffle=True)
    test_iter = torch.utils.data.DataLoader(
        torchvision.datasets.ImageFolder(os.path.join(data_dir, 'test'),
                                         transform=test_augs),
        batch_size=batch_size)
    devices = d2l.try_all_gpus()
    loss = nn.CrossEntropyLoss(reduction="none")
    # 有点问题。。。
    if param_group:
        # here ？？？
        params_1x = [
            param for name, param in net.named_parameters()
            if name not in ["fc.weight", "fc.bias"]]
        trainer = torch.optim.SGD([{
            'params': params_1x}, {
                'params': net.fc.parameters(),
                'lr': learning_rate * 10}], lr=learning_rate,
                                  weight_decay=0.001)
    else:
        trainer = torch.optim.SGD(net.parameters(), lr=learning_rate,
                                  weight_decay=0.001)
    d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs,
                   devices)
```

- 迁移学习将从源数据集中学到的知识“迁移”到目标数据集，微调是迁移学习的常见技巧。
- 除输出层外，目标模型从源模型中复制所有模型设计及其参数，并根据目标数据集对这些参数进行微调。但是，目标模型的输出层需要从头开始训练。
- 通常，微调参数使用较小的学习率，而从头开始训练输出层可以使用更大的学习率。

## 图像增广

> 我们可以通过对图像进行随机裁剪，使物体以不同的比例出现在图像的不同位置。 这也可以降低模型对目标位置的敏感性。

```python
# 辅助函数
def apply(img, aug, num_rows=2, num_cols=4, scale=1.5):
    Y = [aug(img) for _ in range(num_rows * num_cols)]
    d2l.show_images(Y, num_rows, num_cols, scale=scale)
```

### 镜像

```python
# 随机水平镜像
apply(img, torchvision.transforms.RandomHorizontalFlip())
```

### 随机裁剪图像

```python
# 随机裁剪，重置大小200x200,面积0.1-1,宽高比0.5-2
apply(img, torchvision.transforms.RandomResizedCrop(
    (200, 200), scale=(0.1, 1), ratio=(0.5, 2)))
```

### 调整亮度、对比度、饱和度和色调

```python
# brightness：亮度，contrast：对比度，hue：色调，saturation：饱和度
apply(img, torchvision.transforms.ColorJitter
(brightness=0.5, contrast=0, saturation=0, hue=0))
```

### 结合多种图像增广方法

```python
augs = torchvision.transforms.Compose([
    torchvision.transforms.RandomHorizontalFlip(),
    torchvision.transforms.ColorJitter(brightness=0.5, contrast=0, saturation=0, hue=0),
    torchvision.transforms.RandomResizedCrop((200, 200), scale=(0.1, 1), ratio=(0.5, 2))
])
apply(img, augs)
```

### 使用图像增广进行训练

>为了在预测过程中得到确切的结果，我们通常对训练样本只进行图像增广，且在预测过程中不使用随机操作的图像增广。 

```python
train_augs = torchvision.transforms.Compose([
    torchvision.transforms.RandomHorizontalFlip(),# 水平变换
    torchvision.transforms.ToTensor()])

test_augs = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])

# 辅助函数
def load_cifar10(is_train, augs, batch_size):
    dataset = torchvision.datasets.CIFAR10(root="../data", train=is_train,
                                           transform=augs, download=True)
    dataloader = torch.utils.data.DataLoader(
        dataset, batch_size=batch_size, shuffle=is_train,
        num_workers=d2l.get_dataloader_workers())
    return dataloader
```

## 目标检测和边界框

> 通常使用*边界框*（bounding box）来描述对象的空间位置。 
>
> 边界框是矩形的，由矩形左上角的 x和 y坐标以及右下角的坐标决定。 另一种常用的边界框表示方法是边界框中心的 (x,y)轴坐标以及框的宽度和高度。

```python
# 转换函数
def box_corner_to_center(boxes):
    """从（左上，右下）转换到（中间，宽度，高度）"""
    x1, y1, x2, y2 = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]
    cx = (x1 + x2) / 2
    cy = (y1 + y2) / 2
    w = x2 - x1
    h = y2 - y1
    boxes = torch.stack((cx, cy, w, h), axis=-1)
    return boxes

def box_center_to_corner(boxes):
    """从（中间，宽度，高度）转换到（左上，右下）"""
    cx, cy, w, h = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]
    x1 = cx - 0.5 * w
    y1 = cy - 0.5 * h
    x2 = cx + 0.5 * w
    y2 = cy + 0.5 * h
    boxes = torch.stack((x1, y1, x2, y2), axis=-1)
    return boxes

#matplotlib可以显示的格式
def bbox_to_rect(bbox, color):
    # 将边界框 (左上x, 左上y, 右下x, 右下y) 格式转换成 matplotlib 格式：
    # ((左上x, 左上y), 宽, 高)
    return d2l.plt.Rectangle(xy=(bbox[0], bbox[1]), width=bbox[2] - bbox[0],
                             height=bbox[3] - bbox[1], fill=False,
                             edgecolor=color, linewidth=2)

# bbox是边界框的英文缩写
cat_bbox = [60.0, 45.0, 378.0, 516.0]
fig = d2l.plt.imshow(img)
fig.axes.add_patch(bbox_to_rect(cat_bbox, 'blue'))
```

## 锚框

>它以每个像素为中心生成多个大小和宽高比（aspect ratio）不同的边界框。

 在实践中，我们只考虑包含 s1 或 r1 的组合：`(s1,r1),(s1,r2)...(s1,rm),(s2,r1),(s3,r1)`等，也就是说，以同一个像素中心的锚框数量是n+m-1，对于整个输入图像，一共生成wh（n+m-1）

？？？





































