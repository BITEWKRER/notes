# 深度学习

## pytorch 安装

1.安装minicoda

2.更换国内源

```
# 北京外国语大学源
conda config --add channels  https://mirrors.bfsu.edu.cn/anaconda/pkgs/main
conda config --add channels  https://mirrors.bfsu.edu.cn/anaconda/pkgs/free
conda config --add channels  https://mirrors.bfsu.edu.cn/anaconda/pkgs/r
conda config --add channels  https://mirrors.bfsu.edu.cn/anaconda/pkgs/pro
conda config --add channels  https://mirrors.bfsu.edu.cn/anaconda/pkgs/msys2
#清华源
conda config --add channels  https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main
conda config --add channels  https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free
conda config --add channels  https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r
conda config --add channels  https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/pro
conda config --add channels  https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2
# 更换默认源
conda config --remove-key channels
```

3.创建pytorch环境

```
conda create -n pytorch python=3.9
```

4.安装pytorch-cuda

```
conda install pytorch torchvision torchaudio cudatoolkit=10.2 -c pytorch
```

## python 基础

> 标量：数值
>
> 标量：一维数组
>
> 矩阵：二维数组
>
> 张量：三维及以上

###  访问元素

![image-20210601143644078](D:\notes\images\image-20210601143644078.png)

###  reshape

> 要改变一个张量的形状而不改变元素数量和元素值，我们可以调用reshape函数。

### 张量连接

> 把多个张量连结在一起

```python
import torch

x = torch.arange(12, dtype=torch.float32).reshape((3, 4)) # 转坏为3x4的矩阵
y = torch.tensor([[1, 2, 3, 4], [2, 3, 4, 5], [4, 5, 6, 7]])
a = torch.cat((x, y), dim=1) # 连接张量
b = torch.cat((x, y), dim=0)
print(a, b)
```

### 点积

```python
x = np.array([1,2,3,4]).reshape(2,2)
y = np.dot(x.T,x)

out:array([[10, 14],
       [14, 20]])
```

 ### 矩阵乘法

```PYTHON
x = torch.tensor([1,2,3,4]).reshape(2,2)
y = torch.mm(x.T,x)

out:array([[10, 14],
       [14, 20]])
```

###  clone

> 将重新分配内存，将A的值进行赋值

### 范数

![image-20210601185017730](D:\notes\images\image-20210601185017730.png)

```python
u = torch.tensor([3.,4.])
u.norm()

out:tensor(5.)
```

### 按特定轴求和

> **axis 取值为0-行,1-列...**
>
> 假设现有矩阵 5x4，则axis=1时，长度为5，axis=0时，长度为4(axis是多少就将特定的轴划掉,三维及以上均适用)
>
> **keepdims=True时，axis所指向的index等于1**，即[2,5,4],axis=2,keepdims=True==>[2,5,1]

```python
a = torch.ones(5,2)
a.shape
a.sum(axis=1)

Out: tensor([2., 2., 2., 2., 2.])
    
a.sum(axis=1,keepdims=True).size()
Out: torch.Size([5, 1])
```

## 求导
### 导数

![image-20210601201958331](D:\notes\images\image-20210601201958331.png)

### 偏导数

![image-20210601210312992](D:\notes\images\image-20210601210312992.png)

### 反向传播

![image-20210602094922162](D:\notes\images\image-20210602094922162.png)

![image-20210602095206436](D:\notes\images\image-20210602095206436.png)

#### 实现

```python
x = torch.arange(4., requires_grad=True)
y = 2 * torch.dot(x, x)
# 计算y关于x的倒数
y.backward(retain_graph=True)
x.grad

out:tensor([4., 4., 4., 4.])
```

### 函数反向传播

> 即使构建函数的计算图需要通过Python控制流(例如，条件、循环或任意函数调用)，我们仍然可以计算得到的变量的梯度

```python
def f(a):
    b = a * 2
    while b.norm() < 100:
        b *= 2
    if b.sum() > 2:
        return b
    else:
        return 100 * b


a = torch.randn(size=(), requires_grad=True)
c = f(a)
c.backward()
a.grad == c / a
out:tensor(True)
```

## N:特征筛选

> 简单的关系更加易于学习，复杂的关系则需要更多的训练数据，因此更难被学习出来。

## 线性回归

### 生成模拟数据

```python
import random
import torch

# 随机生成模拟数据
def syn_data(w, b, num):
    x = torch.normal(0, 1, (num, len(w)))
    y = torch.matmul(x, w) + b
    y += torch.normal(0, 0.01, y.shape)
    return x, y.reshape((-1, 1))

true_w = torch.tensor([2, -3.4])
true_b = 4.2
f, l = syn_data(true_w, true_b, 1000)
```

### 实现

```python
# 每次返回batch——size个数据
def iter_data(batch_size, f, l):
    # 特征数据大小
    num = len(f)
    # 从零开始的index
    index = list(range(num))
    # 打乱标签
    random.shuffle(index)
	
    for i in range(0, num, batch_size):
        batch_index = torch.tensor(index[i:min(i + batch_size, num)])
        yield f[batch_index], l[batch_index]

# net model  
def lingre(x, w, b):
    return torch.mm(x, w) + b

# 损失函数
def loss(y_hat, y):
    return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2

#梯度下降
def sgd(params, lr, batch_size):
    # 更新时无需计算梯度
    with torch.no_grad():
        for param in params:
            param -= lr * param.grad / batch_size
            param.grad.zero_()

# 初始化w，b
w = torch.normal(0, 1, size=(2, 1), requires_grad=True)
b = torch.zeros(1, requires_grad=True)

lr = 0.03
net = lingre
epoch = 3 
batch_size = 100
for e in range(epoch):
    for x, y in iter_data(batch_size, f, l):
        ll = loss(net(x, w, b), y)
        ll.sum().backward()
        sgd([w, b], lr, batch_size)
    with torch.no_grad():
        train_l = loss(net(f, w, b), l)
        print(f'epoch {e + 1}, loss {float(train_l.mean()):f}')

print(f'w的估计误差: {true_w - w.reshape(true_w.shape)}')
print(f'b的估计误差: {true_b - b}')
```

## 参数绑定

### 共享参数

> 用于参数不同层共享

```python
    shared = nn.Linear(8, 8)
    net = nn.Sequential(nn.Linear(8, 8), shared, nn.RuLU(), shared, nn.RuLU()
```

### 读写文件

#### 保存张量

```python
# 写入数组
x = torch.arange(12).reshape(3,4)
y = torch.arange(12).reshape(3,4)
# 单个tensor
torch.save(x,"x-file")
# 多个tensor
torch.save([x,y],"x-file")
# 读取
x = torch.load("x-file")
```

#### 模型参数

```python
# 保存模型参数
def net():
    return nn.Sequential(nn.Linear(in_features, 1))
# 获取所有层的参数
data = net.state_dict()
torch.save(data,"net.params")
# 加载模型参数
# 新定义一个模型
x = net()
x.load_state_dict(torch.load("net.params"))
```

## gpu使用

> gpu使用率如果在50%以下，则可以认为模型选择不好

```python
#cpu运算，默认
torch.deivce('cpu')
# 单个gpu
torch.cuda.device('cuda')
# 多个gpu,使用第一个gpu
torch.cuda.device('cuda:1')
# 查看gpu数量
torch.cuda.device_count()
```

### gpu、cpu选择

```python
def try_gpu(i = 0):
    if torch.cuda.device_count() >= i+1:
        return torch.device(f'cuda:{i}')
    return torch.device('cpu')
# 张量创建
x = torch.ones(2,3,device=try_gpu())
# 不同张量运算时，必须放在同一个gpu上
y = torch.ones(2,3,device=try_gpu(1))
z = x.cuda(1)
print(x+y)
# 神经网络保存模型参数到gpu
# 方法一
def net():
    return nn.Sequential(nn.Linear(in_features, 1)).cuda()
#.cuda() Moves all model parameters and buffers to the GPU.
# 方法二
x = net()
x.to(device = try_gpu())
# 验证
x[0].weight.data.device
```

## 丢失:手写数字分类

### cee实现

> 目的：拿出真实标号的预测值
>
> 创建一个预测数据y_ hat， 其中包含2个样本在3个类别的预测概率，使用y作为y_ hat中概率的索引

```python
y = torch.tensor([0, 2])  # 样本编号
y_hat = torch.tensor([[0.1, 0.3, 0.6], [0.2, 0.3, 0.5]])
y_hat[[0, 1], y] # 即取y0的0号元素为0，则y_hat = 0.1
out:tensor([0.1000, 0.5000])
```

```python
def cee(y_hat,y):
	return -torch.log(y_hat[range(len(y)),y])
```

### 实现

> 心得：通过训练，不断地调整net内部的w，b实现拟合

```python
import random
import torch
from torch.utils import data
import torchvision
from torchvision import transforms

#加载数据集
def get_fashion_mnist_labels(labels):
    """返回Fashion-MNIST数据集的文本标签。"""
    text_labels = [
        't-shirt', 'trouser', 'pullover', 'dress', 'coat', 'sandal', 'shirt',
        'sneaker', 'bag', 'ankle boot']
    return [text_labels[int(i)] for i in labels]


def get_dataloader_workers():  # @save
    """使用8个进程来读取数据。"""
    return 8


def load_data_fashion_mnist(batch_size, resize=None):  # @save
    """下载Fashion-MNIST数据集，然后将其加载到内存中。"""
    trans = [transforms.ToTensor()]
    if resize:
        trans.insert(0, transforms.Resize(resize))
    trans = transforms.Compose(trans)
    mnist_train = torchvision.datasets.FashionMNIST(root="./data",
                                                    train=True,
                                                    transform=trans,
                                                    download=True)
    mnist_test = torchvision.datasets.FashionMNIST(root="./data",
                                                   train=False,
                                                   transform=trans,
                                                   download=True)
    return (data.DataLoader(mnist_train, batch_size, shuffle=True,
                            num_workers=get_dataloader_workers()),
            data.DataLoader(mnist_test, batch_size, shuffle=False,
                            num_workers=get_dataloader_workers()))


def net(x):
    z = torch.matmul(x.reshape(-1, w.shape[0]), w) + b
    return softmax(z)


def softmax(x):
    c = torch.max(x)
    exp = torch.exp(c - x)
    return exp / exp.sum(1, keepdim=True)


def cross_entropy(y_hat, y):
    return -torch.log(y_hat[range(len(y_hat)), y])


def accuracy(y_hat, y):
    """计算预测正确的数量。"""
    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:
        y_hat = y_hat.argmax(axis=1)
    cmp = y_hat.type(y.dtype) == y
    return float(cmp.type(y.dtype).sum())


def evaluate_accuracy(net, data_iter):  # @save
    """计算在指定数据集上模型的精度。"""
    if isinstance(net, torch.nn.Module):
        net.eval()  # 将模型设置为评估模式
    metric = Accumulator(2)  # 正确预测数、预测总数
    for X, y in data_iter:
        metric.add(accuracy(net(X), y), y.numel())
    return metric[0] / metric[1]


class Accumulator:  # @save
    """在`n`个变量上累加。"""

    def __init__(self, n):
        self.data = [0.0] * n

    def add(self, *args):
        self.data = [a + float(b) for a, b in zip(self.data, args)]

    def reset(self):
        self.data = [0.0] * len(self.data)

    def __getitem__(self, idx):
        return self.data[idx]


def train_epoch_ch3(net, train_iter, loss, updater):  # @save
    """训练模型一个迭代周期（定义见第3章）。"""
    # 将模型设置为训练模式
    if isinstance(net, torch.nn.Module):
        net.train()
    # 训练损失总和、训练准确度总和、样本数
    metric = Accumulator(3)
    for X, y in train_iter:
        # 计算梯度并更新参数
        y_hat = net(X)
        l = loss(y_hat, y)
        # 梯度更新 ？？？
        if isinstance(updater, torch.optim.Optimizer):
            # 使用PyTorch内置的优化器和损失函数
            updater.zero_grad()
            l.backward()
            updater.step()
            metric.add(
                float(l) * len(y), accuracy(y_hat, y),
                y.size().numel())
        else:
            # 使用定制的优化器和损失函数
            l.sum().backward()
            updater(X.shape[0])
            metric.add(float(l.sum()), accuracy(y_hat, y), y.numel())
    # 返回训练损失和训练准确率
    return metric[0] / metric[2], metric[1] / metric[2]


def train_ch3(net, train_iter, test_iter, loss, num_epochs, updater):  # @save

    train_metrics = None
    for epoch in range(num_epochs):
        train_metrics = train_epoch_ch3(net, train_iter, loss, updater)
        test_acc = evaluate_accuracy(net, test_iter)

    train_loss, train_acc = train_metrics
    print(train_loss, train_acc)


def updater(batch_size):
    return sgd([w, b], lr, batch_size)


# 梯度下降
def sgd(params, lr, batch_size):
    # 更新时无需计算梯度
    with torch.no_grad():
        for param in params:
            param -= lr * param.grad / batch_size
            param.grad.zero_()


def predict_ch3(net, test_iter, n=6):
    """预测标签（定义见第3章）。"""
    for x, y in test_iter:
        break
    trues = get_fashion_mnist_labels(y)
    preds = get_fashion_mnist_labels(net(x).argmax(axis=1))
    titles = [true + '\n' + pred for true, pred in zip(trues, preds)]
    print(titles)


if __name__ == '__main__':
    num_input = 784
    num_output = 10
    # 加载数据
    train_iter, test_iter = load_data_fashion_mnist(32, resize=28)
    # 初始化权重
    w = torch.normal(0, 0.01, size=(num_input, num_output), requires_grad=True)
    b = torch.zeros(num_output, requires_grad=True)
    lr = 0.1
    num_epochs = 10
    train_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, updater)
    predict_ch3(net, test_iter)
```

## R:房价预测

### 下载数据

```python
import hashlib
import os
import tarfile
import zipfile
import requests
import numpy as np
import pandas as pd
import torch
from torch import nn

# @save
DATA_HUB = dict()
DATA_URL = 'http://d2l-data.s3-accelerate.amazonaws.com/'


def download(name, cache_dir=os.path.join('../..', 'data')):  # @save
    """下载一个DATA_HUB中的文件，返回本地文件名。"""
    assert name in DATA_HUB, f"{name} 不存在于 {DATA_HUB}."
    url, sha1_hash = DATA_HUB[name]
    os.makedirs(cache_dir, exist_ok=True)
    fname = os.path.join(cache_dir, url.split('/')[-1])
    if os.path.exists(fname):
        sha1 = hashlib.sha1()
        with open(fname, 'rb') as f:
            while True:
                data = f.read(1048576)
                if not data:
                    break
                sha1.update(data)
        if sha1.hexdigest() == sha1_hash:
            return fname  # Hit cache
    print(f'正在从{url}下载{fname}...')
    r = requests.get(url, stream=True, verify=True)
    with open(fname, 'wb') as f:
        f.write(r.content)
    return fname


def download_extract(name, folder=None):  # @save
    """下载并解压zip/tar文件。"""
    fname = download(name)
    base_dir = os.path.dirname(fname)
    data_dir, ext = os.path.splitext(fname)
    if ext == '.zip':
        fp = zipfile.ZipFile(fname, 'r')
    elif ext in ('.tar', '.gz'):
        fp = tarfile.open(fname, 'r')
    else:
        assert False, '只有zip/tar文件可以被解压缩。'
    fp.extractall(base_dir)
    return os.path.join(base_dir, folder) if folder else data_dir


def download_all():  # @save
    """下载DATA_HUB中的所有文件。"""
    for name in DATA_HUB:
        download(name)
```

### 数据处理

```python
def load_data:
    DATA_HUB['kaggle_house_train'] = (  # @save
        DATA_URL + 'kaggle_house_pred_train.csv',
        '585e9cc93e70b39160e7921475f9bcd7d31219ce')

    DATA_HUB['kaggle_house_test'] = (  # @save
        DATA_URL + 'kaggle_house_pred_test.csv',
        'fa19780a7b011d9b009e8bff8e99922a8ee2eb90')
    train_data = pd.read_csv(download('kaggle_house_train'))
    test_data = pd.read_csv(download('kaggle_house_test'))
    print(train_data.shape)
    print(test_data.shape)
    # print(train_data.iloc[0:4, [0, 1, 2, 3, -3, -2, -1]])

    # 数据预处理-裁剪数据
    all_features = pd.concat((train_data.iloc[:, 1:-1], test_data.iloc[:, 1:]))
    print(all_features.iloc[0:4, [0, 1, 2, 3, -3, -2, -1]])
    # 归一化
    numeric_features = all_features.dtypes[all_features.dtypes != 'object'].index
    all_features[numeric_features] = all_features[numeric_features].apply(lambda x: (x - x.mean()) / (x.std()))
    # 在标准化数据之后，所有数据都意味着消失，因此我们可以将缺失值设置为0
    all_features[numeric_features] = all_features[numeric_features].fillna(0)

    # 处理字符串
    all_features = pd.get_dummies(all_features, dummy_na=True)
    print(all_features.shape)

    # 将pandas 转换为numpy
    n_train = train_data.shape[0]
    # 将训练数据集和测试数据集分开，转换为二维tensor float类型
    train_features = torch.tensor(all_features[:n_train].values, dtype=torch.float32)
    test_features = torch.tensor(all_features[n_train:].values, dtype=torch.float32)
    # train_data.info() 查看 csv文件的key
    # train_labels = torch.tensor(train_data['SalePrice'].values.reshape(-1, 1), dtype=torch.float32)
    train_labels = torch.tensor(train_data.SalePrice.values.reshape(-1, 1), dtype=torch.float32)
    return train_features, test_features, train_labels
```

### 定义模型

```python
def get_net(in_features):
    # 单层模型
    return nn.Sequential(nn.Linear(in_features, 1))
```

### log_rmse

>QA:已经使用了loss函数为什么还要套用下面的公式？

![image-20210605101636452](D:\notes\images\image-20210605101636452.png)

```python
def log_rmse(net, features, labels):
    # 为了在取对数时进一步稳定该值，将小于1的值设置为1
    # torch.clamp(input, min, max, out=None) → Tensor 将值固定到1和inf之间
    clipped_preds = torch.clamp(net(features), 1, float('inf'))
    # 套用公式计算
    rmse = torch.sqrt(loss(torch.log(clipped_preds), torch.log(labels)))
    return rmse.item()
```

### 训练

```python
def train(net, train_features, train_labels, test_features, test_labels,
          num_epochs, learning_rate, weight_decay, batch_size):
    train_ls, test_ls = [], []
    train_iter = d2l.load_array((train_features, train_labels), batch_size)
    # 这里使用的是Adam优化算法
    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate,
                                 weight_decay=weight_decay)
    for epoch in range(num_epochs):
        for X, y in train_iter:
            optimizer.zero_grad()
            l = loss(net(X), y)
            l.backward()
            optimizer.step()
        train_ls.append(log_rmse(net, train_features, train_labels))
        if test_labels is not None:
            test_ls.append(log_rmse(net, test_features, test_labels))
    return train_ls, test_ls
```

### k折模型及训练，验证误差

```python
# 在 K 折交叉验证过程中返回第 i 折的数据。它选择第 i 个切片作为验证数据，其余部分作为训练数据。
def get_k_fold_data(k, i, X, y):
    assert k > 1
    # 求根据k得到分数大小。向下除法取整 eg：784//2 = 392
    fold_size = X.shape[0] // k
    X_train, y_train = None, None
    for j in range(k):
        # slice(start, stop[, step])
        # （0，392） ，得到一个数据样本集
        idx = slice(j * fold_size, (j + 1) * fold_size)
        # 得到具体数据
        X_part, y_part = X[idx, :], y[idx]
        # j==i时为验证数据集
        if j == i:
            X_valid, y_valid = X_part, y_part
        # 否则就是训练数据集，当第一次为空时，初始化数据集，后面不为空直接cat数据集
        elif X_train is None:
            X_train, y_train = X_part, y_part
        else:
            # （[tensor，tensor]，dim）
            X_train = torch.cat([X_train, X_part], 0)
            y_train = torch.cat([y_train, y_part], 0) 
    return X_train, y_train, X_valid, y_valid

# 在 K 折交叉验证中训练 K 次后，返回训练和验证误差的平均值。
def k_fold(k, X_train, y_train, num_epochs, learning_rate, weight_decay, batch_size):
    train_l_sum, valid_l_sum = 0, 0
    for i in range(k):
        data = get_k_fold_data(k, i, X_train, y_train)
        # 新的网络
        net = get_net()
        # *data ：传入的参数存入同一个元组中
        train_ls, valid_ls = train(net, *data, num_epochs, learning_rate, weight_decay, batch_size)
        # 求训练误差和和验证误差和
        train_l_sum += train_ls[-1]
        valid_l_sum += valid_ls[-1]

        print(f'fold {i + 1}, train log rmse {float(train_ls[-1]):f}, '
              f'valid log rmse {float(valid_ls[-1]):f}')
        # 求平均训练误差和验证误差
    return train_l_sum / k, valid_l_sum / k
```

### 运行

```python
if __name__ == '__main__':
    train_features, test_features, train_labels = load_data()
    # 损失函数
    loss = nn.MSELoss()
    # 内部特征
    in_features = train_features.shape[1]
    # 训练
    # 超参数
    k, num_epochs, lr, weight_decay, batch_size = 5, 1000, 5, 0, 64
    train_l, valid_l = k_fold(k, train_features, train_labels, num_epochs, lr, weight_decay, batch_size)
    print(f'{k}-折验证: 平均训练log rmse: {float(train_l):f}, 'f'平均验证log rmse: {float(valid_l):f}')
```

## R：树叶分类

### 数据加载：重新dataset

> [参考地址](https://www.pytorchtutorial.com/pytorch-custom-dataset-examples/)
>
> 数据类型：csv文件包含图片地址及label，还有图片文件夹
>
> 存在问题：label无法标记

```python
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
import pandas as pd
from PIL import Image

path = 'D:\系统备份\学习\深度学习\DL\classify'
size = 14000


class dataSet(Dataset):
    def __init__(self, csv_path, trans, train=True):
        self.trans = trans
        # 读取csv文件
        self.csv_data = pd.read_csv(csv_path)
        # csv 文件列
        if train:
            self.img_name_list = self.csv_data.iloc[:size, 0]
            self.img_label_list = self.csv_data.iloc[:size, 1]
        else:
            self.img_name_list = self.csv_data.iloc[size:, 0]
            self.img_label_list = self.csv_data.iloc[size:, 1]
        # dataset长度
        self.len = len(self.csv_data.index)

    def __getitem__(self, idx):
        img = self.img_name_list[idx]
        label = self.img_label_list[idx]
        # to tensor
        readImg = Image.open(path + '/' + img)
        fix_img = self.trans(readImg)
        return fix_img, label

    def __len__(self):
        return self.len


def dataloader(dataset, batch_size, shuffle=True):
    return DataLoader(dataset, batch_size, shuffle)


def load(csv_name):
    trans = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()])
    train = dataSet(path + csv_name, trans, True)
    test = dataSet(path + csv_name, trans, False)

    train_iter = dataloader(train, 64)
    test_iter = dataloader(test, 64)
    return train_iter, test_iter
```

### 数据加载：数据拆分（used）

> 将同一类的图片存入同意文件夹下

```python
import os
import shutil
import pandas as pd
import torch
import torchvision
from torch import nn
from d2l import torch as d2l

data_dir = 'D:\系统备份\学习\深度学习\DL\classify'


def read_csv_labels(fname):
    """读取 `fname` 来给标签字典返回一个文件名。"""
    with open(fname, 'r') as f:
        # 跳过文件头行 (列名)
        lines = f.readlines()[1:]
    tokens = [l.rstrip().split(',') for l in lines]
    return dict(((name, label) for name, label in tokens))


def copyfile(filename, target_dir):
    """将文件复制到目标目录。"""
    os.makedirs(target_dir, exist_ok=True)
    shutil.copy(filename, target_dir)


def reorg_train_valid(data_dir, labels, valid_ratio):
    # 训练数据集中示例最少的类别中的示例数
    n = collections.Counter(labels.values()).most_common()[-1][1]
    # 验证集中每个类别的示例数
    n_valid_per_label = max(1, math.floor(n * valid_ratio))
    label_count = {}
    for train_file in os.listdir(os.path.join(data_dir, 'images')):
        print(train_file)
        label = labels['images/' + train_file]
        fname = os.path.join(data_dir, 'images', train_file)
        copyfile(fname, os.path.join(data_dir, 'train_valid_test', 'train_valid', label))
        if label not in label_count or label_count[label] < n_valid_per_label:
            copyfile(fname, os.path.join(data_dir, 'train_valid_test', 'valid', label))
            label_count[label] = label_count.get(label, 0) + 1
        else:
            copyfile(fname, os.path.join(data_dir, 'train_valid_test', 'train', label))
    return n_valid_per_label


# @save
def reorg_test(data_dir):
    for test_file in os.listdir(os.path.join(data_dir, 'test')):
        copyfile(os.path.join(data_dir, 'test', test_file),
                 os.path.join(data_dir, 'train_valid_test', 'test', 'unknown'))


def reorg_cifar10_data(data_dir, valid_ratio):
    labels = read_csv_labels(os.path.join(data_dir, 'train_1.csv'))
    n = reorg_train_valid(data_dir, labels, valid_ratio)
    reorg_test(data_dir)


batch_size = 64
valid_ratio = 0.1
reorg_cifar10_data(data_dir, valid_ratio)
```

























